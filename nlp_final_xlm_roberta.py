# -*- coding: utf-8 -*-
"""nlp-final-xlm-roberta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ykyJDjnWOe3gyv0VPcx9GzIF5NLPg0Er
"""

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
except ValueError:
    strategy = tf.distribute.get_strategy() # for CPU and single GPU
    print('Number of replicas:', strategy.num_replicas_in_sync)

train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')
#test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')

train.info()

train.describe(include='all')

train=train.drop('id',axis=1)
train.head()

train=train.drop('language',axis=1)
#test=test.drop('language',axis=1)
train.head(5)

sns.countplot(x='lang_abv', data=train)

plt.figure(figsize=(9,9))
train.groupby('lang_abv').size().plot(kind='pie', autopct='%1.1f%%')

from sklearn.model_selection import train_test_split

train, test = train_test_split(train, test_size=0.2, random_state=42)
# Here, `train_data` will contain 80% of the data, and `test_data` will contain the remaining 20%

from transformers import BertTokenizer, TFBertModel, TFAutoModel,AutoTokenizer
model_name ='joeddav/xlm-roberta-large-xnli'
tokenizer = AutoTokenizer.from_pretrained(model_name,use_auth_token='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')

def encode_premise_sentence(s):
    tokens=[]
    tokens.append('[CLS]')
    tokens+=list(tokenizer.tokenize(s))
    return tokenizer.convert_tokens_to_ids(tokens)

def encode_hypo_sentence(s):
    tokens=[]
    tokens.append('[sep]')
    tokens+=list(tokenizer.tokenize(s))
    tokens.append('[sep]')
    return tokenizer.convert_tokens_to_ids(tokens)

tokenized=[]
for i in range(len(train)):
    pre=encode_premise_sentence(train['premise'][i])
    hyp=encode_hypo_sentence(train['hypothesis'][i])
    tokenized.append(pre+hyp)
train['tokenized']=tokenized
train.head()

mask=[]
for i in range(len(train)):
    padded_seq=tokenizer(train['premise'][i],train['hypothesis'][i], padding=True,add_special_tokens = True)
    mask.append(padded_seq)
train['masked'] = mask
train.head(5)
# print(mask[0])

max_len=237
def build_model():
    bert_encoder = TFAutoModel.from_pretrained('joeddav/xlm-roberta-large-xnli', use_auth_token='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')
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    embedding = bert_encoder([input_word_ids, input_mask])[0]
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model

def input_convert(data):
    inputs={
        'input_word_ids':[],
        'input_mask':[]
    }
    for each in data:
        inputs['input_word_ids'].append(each['input_ids'])
        inputs['input_mask'].append(each['attention_mask'])
        
    inputs['input_word_ids']= tf.ragged.constant( inputs['input_word_ids']).to_tensor()
    inputs['input_mask']= tf.ragged.constant( inputs['input_mask']).to_tensor()
    return inputs

train_input=input_convert(train['masked'].values)
for key in train_input.keys():
    train_input[key] = train_input[key][:,:max_len]

early_stop = tf.keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)
with strategy.scope():
    model = build_model()
    model.summary()
    model.fit(train_input, train['label'].values, epochs = 5, verbose = 1, batch_size = 4, validation_split = 0.1 ,callbacks=[early_stop])

mask=[]
for i in range(len(test)):
    padded_seq=tokenizer(test['premise'][i],test['hypothesis'][i],
                        padding=True,add_special_tokens =True)
    mask.append(padded_seq)
test['masked']=mask
test.head()

test_input=input_convert(test['masked'].values)
for key in test_input.keys():
    test_input[key]=test_input[key][:,:max_len]

predictions=[np.argmax(i) for i in model.predict(test_input)]

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


accuracy = accuracy_score(test.label.values, predictions)
precision = precision_score(test.label.values, predictions, average='macro')
recall = recall_score(test.label.values, predictions, average='macro')
f1 = f1_score(test.label.values, predictions, average='macro')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 score: {f1:.4f}")

