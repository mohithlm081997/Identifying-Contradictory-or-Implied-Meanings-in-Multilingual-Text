# -*- coding: utf-8 -*-
"""NLP_Final_BERT_With_Metrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zHolC7oDPq6BVE4l223yQDsOnWcriRAT
"""

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from transformers import BertTokenizer, TFBertModel

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

os.environ["WANDB_API_KEY"] = "0" ## to silence warning

train = pd.read_csv("/kaggle/input/contradictory-my-dear-watson/train.csv")
#test = pd.read_csv("/kaggle/input/contradictory-my-dear-watson/test.csv")

train

train.shape

train.info()

train.isna().sum()

"""Analyse train"""

sns.displot(train['label'], kde=True)

train['label'].value_counts()

sns.displot(train['lang_abv'], kde=True)

sns.displot(test['lang_abv'], kde=True)

labels, frequencies = np.unique(train.language.values, return_counts = True)

plt.figure(figsize = (10,10))
plt.pie(frequencies,labels = labels, autopct = '%1.1f%%')
plt.show()

train

from sklearn.model_selection import train_test_split

train, test = train_test_split(train, test_size=0.2, random_state=42)
# Here, `train_data` will contain 80% of the data, and `test_data` will contain the remaining 20%

train.shape

test.shape

# Define global variables
MODEL_NAME = 'bert-base-multilingual-cased'
TOKENIZER = BertTokenizer.from_pretrained(MODEL_NAME)

def encode_sentence(s, tokenizer):
    """Encodes a sentence using the given tokenizer."""
    tokens = list(tokenizer.tokenize(s))
    tokens.append('[SEP]')
    return tokenizer.convert_tokens_to_ids(tokens)

def bert_encode(premises, hypotheses, tokenizer):
    """
    Encodes the given premises and hypotheses using a BERT tokenizer,
    and returns a dictionary with the inputs required for the BERT model.
    """
    num_examples = len(hypotheses)

    sentence1 = tf.ragged.constant([
        encode_sentence(s, tokenizer)
        for s in np.array(hypotheses)])
    sentence2 = tf.ragged.constant([
        encode_sentence(s, tokenizer)
        for s in np.array(premises)])

    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]
    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)

    input_mask = tf.ones_like(input_word_ids).to_tensor()

    type_cls = tf.zeros_like(cls)
    type_s1 = tf.zeros_like(sentence1)
    type_s2 = tf.ones_like(sentence2)
    input_type_ids = tf.concat(
        [type_cls, type_s1, type_s2], axis=-1).to_tensor()

    inputs = {
        'input_word_ids': input_word_ids.to_tensor(),
        'input_mask': input_mask,
        'input_type_ids': input_type_ids}

    return inputs

def build_model(model_name, learning_rate):
    """
    Builds a BERT model with a dense output layer and returns it.
    """
    bert_encoder = TFBertModel.from_pretrained(model_name, use_auth_token='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')
    
    input_word_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name="input_word_ids")
    input_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name="input_mask")
    input_type_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name="input_type_ids")
    
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])
    
    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)

    model.compile(tf.keras.optimizers.Adam(learning_rate=learning_rate), 
                  loss='sparse_categorical_crossentropy', 
                  metrics=['accuracy'])
    
    return model

train_input = bert_encode(train.premise.values, train.hypothesis.values, TOKENIZER)
model = build_model(MODEL_NAME, learning_rate=1e-5)

model.fit(train_input, train.label.values, epochs = 5, verbose = 1, batch_size = 16, validation_split = 0.1)

test_input = bert_encode(test.premise.values, test.hypothesis.values, TOKENIZER)

predictions = [np.argmax(i) for i in model.predict(test_input)]

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


accuracy = accuracy_score(test.label.values, predictions)
precision = precision_score(test.label.values, predictions, average='macro')
recall = recall_score(test.label.values, predictions, average='macro')
f1 = f1_score(test.label.values, predictions, average='macro')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 score: {f1:.4f}")